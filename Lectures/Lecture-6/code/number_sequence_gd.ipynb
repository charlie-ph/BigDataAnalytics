{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number Sequence \n",
    "Consider that we observed the following sequence of integer numbers 18, 22, 45, 49, 86. What is the next number? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark Data Partitioning Example\") \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create number array\n",
    "num_arr = np.array([18, 22, 45, 49, 86])\n",
    "\n",
    "f = num_arr.copy()\n",
    "t = np.arange(1, num_arr.size + 1)\n",
    "plt.plot(t, f, 'o', markersize=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the parameter m using sklearn. \n",
    "The fit() method returns an instance of the LinearRegression class, which contains the slope of the line in its coef_ attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14.96363636]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X = np.stack([t], axis=1)\n",
    "reg = LinearRegression(fit_intercept=False).fit(X, f)\n",
    "print(reg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the parameter using linear algebra\n",
    "Calculate the coefficients of a linear regression model using the normal equation approach\n",
    "\n",
    "$$\n",
    "m = (X^T \\cdot X)^{-1} \\cdot X^T \\cdot f\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $m$ represents the estimated coefficients of the linear regression model.\n",
    "- $X$ is the matrix of input variables.\n",
    "- $X^T$ denotes the transpose of $X$.\n",
    "- $f$ is the vector of dependent variable values.\n",
    "- $(X^T \\cdot X)^{-1}$ represents the inverse of the matrix product $X^T \\cdot X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14.96363636]\n"
     ]
    }
   ],
   "source": [
    "# Let use solve this also with the exact linear algebra solution. \n",
    "m = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(f)\n",
    "\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caculating the parameter m using Gradient Descent\n",
    "1. Calculate the prediction with current regression coefficients:\n",
    "   - Prediction: $f_{\\text{prediction}} = m_{\\text{current}} \\cdot t$\n",
    "\n",
    "2. Compute the cost (for monitoring purposes):\n",
    "   - Cost: $cost = \\sum (f - f_{\\text{prediction}})^2$\n",
    "\n",
    "3. Calculation of the gradient:\n",
    "   - Gradient: $m_{\\text{gradient}} = \\frac{-1}{n} \\sum (t \\cdot (f - f_{\\text{prediction}}))$\n",
    "\n",
    "4. Update of the regression coefficient:\n",
    "   - Updated coefficient: $m_{\\text{current}} = m_{\\text{current}} - \\text{learningRate} \\cdot m_{\\text{gradient}}$\n",
    "\n",
    "Here, the variable $n$ represents the number of data points in the dataset, $y$ is the dependent variable, $x_1$ is the independent variable, $m_{\\text{current}}$ is the current value of the coefficient, and $\\text{learningRate}$ is the learning rate (step size) used in the gradient descent algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 m =  0  Cost =  12630\n",
      "1 m =  0.0016460000000000003  Cost =  12627.29083301238\n",
      "2 m =  0.0032918189400000008  Cost =  12624.582262008718\n",
      "3 m =  0.004937456839916601  Cost =  12621.8742868579\n",
      "4 m =  0.00658291371966421  Cost =  12619.166907428851\n",
      "5 m =  0.008228189599155047  Cost =  12616.460123590516\n",
      "6 m =  0.009873284498299141  Cost =  12613.753935211871\n",
      "7 m =  0.011518198437004329  Cost =  12611.048342161928\n",
      "8 m =  0.013162931435176259  Cost =  12608.343344309718\n",
      "9 m =  0.01480748351271839  Cost =  12605.638941524305\n",
      "10 m =  0.016451854689531992  Cost =  12602.935133674779\n",
      "11 m =  0.018096044985516143  Cost =  12600.231920630267\n",
      "12 m =  0.019740054420567737  Cost =  12597.529302259916\n",
      "13 m =  0.021383883014581474  Cost =  12594.827278432902\n",
      "14 m =  0.02302753078744987  Cost =  12592.125849018437\n",
      "15 m =  0.02467099775906325  Cost =  12589.425013885757\n",
      "16 m =  0.026314283949309753  Cost =  12586.724772904123\n",
      "17 m =  0.02795738937807533  Cost =  12584.025125942833\n",
      "18 m =  0.029600314065243744  Cost =  12581.326072871212\n",
      "19 m =  0.031243058030696567  Cost =  12578.627613558605\n",
      "20 m =  0.03288562129431319  Cost =  12575.929747874396\n",
      "21 m =  0.03452800387597081  Cost =  12573.232475687993\n",
      "22 m =  0.036170205795544454  Cost =  12570.535796868833\n",
      "23 m =  0.03781222707290694  Cost =  12567.839711286386\n",
      "24 m =  0.03945406772792892  Cost =  12565.144218810145\n",
      "25 m =  0.04109572778047885  Cost =  12562.449319309631\n",
      "26 m =  0.042737207250422994  Cost =  12559.755012654401\n",
      "27 m =  0.044378506157625446  Cost =  12557.061298714032\n",
      "28 m =  0.046019624521948105  Cost =  12554.368177358136\n",
      "29 m =  0.04766056236325069  Cost =  12551.675648456352\n"
     ]
    }
   ],
   "source": [
    "learningRate = 0.00001\n",
    "numIteration = 30 \n",
    "m_current = 0\n",
    "n = float(f.size)\n",
    "\n",
    "# Let's start with main iterative part of gradient descent algorithm \n",
    "for i in range(numIteration):\n",
    "    # Calculate the prediction with current regression coefficients. \n",
    "    f_prediction = m_current * t \n",
    "    # Compute cost for monitoring \n",
    "    cost= sum (( f - f_prediction)**2)\n",
    "    # Calculate gradients. \n",
    "    m_gradient = (-1.0/n) * sum (t * (f - f_prediction) )\n",
    "    print(i,\"m = \", m_current, \" Cost = \", cost)\n",
    "    # Update the weights - Regression Coefficients \n",
    "    m_current = m_current - learningRate * m_gradient "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the parameter m using Gradient Descent in PySpark\n",
    "- `num_rdd` contains tuples with the independent variable $x$ as the first element (`x[0]`) and the dependent variable $f$ as the second element (`x[1]`). \n",
    "- `size` represents the size of the RDD\n",
    "- `learningRate` represents the learning rate (step size) used in the gradient descent algorithm.\n",
    "- `m` represents the current value of the coefficient.\n",
    "- `gradient` represents the gradient of the cost function.\n",
    "- `cost` represents the value of the current cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 18), (2, 22), (3, 45), (4, 49), (5, 86)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create number RDD as tuples of (1, 18),(2, 22),(3, 45),(4, 49),(5, 86) \n",
    "# that can be fitted on a line with the equation:\n",
    "# f(t) = m × t \n",
    "num_rdd = sc.parallelize(zip(t,f))\n",
    "num_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 m =  0  Cost =  1263.0\n",
      "1 m =  0.0016460000000000003  Cost =  1262.729083301238\n",
      "2 m =  0.0032918189400000008  Cost =  1262.4582262008719\n",
      "3 m =  0.004937456839916601  Cost =  1262.1874286857899\n",
      "4 m =  0.00658291371966421  Cost =  1261.916690742885\n",
      "5 m =  0.008228189599155047  Cost =  1261.6460123590516\n",
      "6 m =  0.009873284498299141  Cost =  1261.375393521187\n",
      "7 m =  0.011518198437004329  Cost =  1261.1048342161928\n",
      "8 m =  0.013162931435176259  Cost =  1260.834334430972\n",
      "9 m =  0.01480748351271839  Cost =  1260.5638941524305\n",
      "10 m =  0.016451854689531992  Cost =  1260.2935133674778\n",
      "11 m =  0.018096044985516143  Cost =  1260.0231920630267\n",
      "12 m =  0.019740054420567737  Cost =  1259.7529302259916\n",
      "13 m =  0.021383883014581474  Cost =  1259.4827278432901\n",
      "14 m =  0.02302753078744987  Cost =  1259.2125849018437\n",
      "15 m =  0.02467099775906325  Cost =  1258.9425013885757\n",
      "16 m =  0.026314283949309753  Cost =  1258.6724772904122\n",
      "17 m =  0.02795738937807533  Cost =  1258.4025125942833\n",
      "18 m =  0.029600314065243744  Cost =  1258.1326072871211\n",
      "19 m =  0.031243058030696567  Cost =  1257.8627613558606\n",
      "20 m =  0.03288562129431319  Cost =  1257.5929747874395\n",
      "21 m =  0.03452800387597081  Cost =  1257.3232475687994\n",
      "22 m =  0.036170205795544454  Cost =  1257.0535796868833\n",
      "23 m =  0.03781222707290694  Cost =  1256.7839711286385\n",
      "24 m =  0.03945406772792892  Cost =  1256.5144218810144\n",
      "25 m =  0.04109572778047885  Cost =  1256.244931930963\n",
      "26 m =  0.042737207250422994  Cost =  1255.97550126544\n",
      "27 m =  0.044378506157625446  Cost =  1255.7061298714032\n",
      "28 m =  0.046019624521948105  Cost =  1255.4368177358135\n",
      "29 m =  0.04766056236325069  Cost =  1255.1675648456353\n"
     ]
    }
   ],
   "source": [
    "# Now we do gradient Decent on our RDD data set. \n",
    "learningRate = 0.00001\n",
    "numIteration = 30 \n",
    "size = f.size\n",
    "m = 0\n",
    "num_rdd.cache()\n",
    "\n",
    "# Gradient Descent algorithm \n",
    "for i in range(numIteration):\n",
    "    gradientCost=num_rdd.map(lambda x: (x[0], (x[1] - x[0] * m))) \\\n",
    "                        .map(lambda x: (x[0] * x[1], x[1] ** 2)) \\\n",
    "                        .reduce(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "    cost= gradientCost[1]/(2*size)\n",
    "    gradient=(-1.0/float(size))* gradientCost[0]\n",
    "    print(i, \"m = \", m, \" Cost = \", cost)\n",
    "    m = m - learningRate * gradient "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Close Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "9db6cbf0fd79f8e79653fe7b0c50b956ca6e525ee712295da3c66f75e4fe96ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
