{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "711908c0",
   "metadata": {},
   "source": [
    "# Spark SQL Basic\n",
    "This is a short tutorial of Spark SQL and Spark DataFrames, including:\n",
    "- DataFrame: creation, transformation and action methods\n",
    "- Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa52f1e9",
   "metadata": {},
   "source": [
    "# DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71824710",
   "metadata": {},
   "source": [
    "## Create a SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32c63597",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://MININT-ML502OK.mshome.net:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Python Spark SQL basic example</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1d3f8e5c950>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d0243c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://MININT-ML502OK.mshome.net:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Python Spark SQL basic example</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=Python Spark SQL basic example>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = spark.sparkContext\n",
    "\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9673cd",
   "metadata": {},
   "source": [
    "## Create a Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83ef231",
   "metadata": {},
   "source": [
    "### From list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecde93f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(drinker='Chris', beer='Berliner', score=5),\n",
       " Row(drinker='Matt', beer='Yuengling', score=10)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = [('Chris', 'Berliner', 5), ('Matt', 'Yuengling', 10)]\n",
    "df = spark.createDataFrame(d, ['drinker', 'beer', 'score'])\n",
    "df.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09001f67",
   "metadata": {},
   "source": [
    "### From RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47f37048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_1='Chris', _2='Berliner', _3=5), Row(_1='Matt', _2='Yuengling', _3=10)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize(d)\n",
    "df = spark.createDataFrame(rdd)\n",
    "df.collect()\n",
    "df = rdd.toDF()\n",
    "df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02ec8319",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(drinker='Chris', beer='Berliner', score=5),\n",
       " Row(drinker='Matt', beer='Yuengling', score=10)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame(rdd, ['drinker', 'beer', 'score'])\n",
    "df.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ceed9ad",
   "metadata": {},
   "source": [
    "### From schema and column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02cc499e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(drinker='Chris', beer='Berliner', score=5),\n",
       " Row(drinker='Matt', beer='Yuengling', score=10)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "schema = StructType([\n",
    "    StructField(\"drinker\", StringType(), True),\n",
    "    StructField(\"beer\", StringType(), True),\n",
    "    StructField(\"score\", IntegerType(), True)])\n",
    "df = spark.createDataFrame(rdd, schema)\n",
    "df.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e7b08c",
   "metadata": {},
   "source": [
    "### From data sources\n",
    "https://spark.apache.org/docs/latest/sql-data-sources.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a08381",
   "metadata": {},
   "source": [
    "#### CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2513aa71",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"path-to-people.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "855f46cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|               _c0|\n",
      "+------------------+\n",
      "|      name;age;job|\n",
      "|Jorge;30;Developer|\n",
      "|  Bob;32;Developer|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Spark SQL provides spark.read().csv(\"file_name\") to read a file or directory of files in CSV format into Spark DataFrame\n",
    "df = spark.read.csv(path)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a880b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---------+\n",
      "|  _c0|_c1|      _c2|\n",
      "+-----+---+---------+\n",
      "| name|age|      job|\n",
      "|Jorge| 30|Developer|\n",
      "|  Bob| 32|Developer|\n",
      "+-----+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read a csv with delimiter, the default delimiter is \",\"\n",
    "df = spark.read.option(\"delimiter\", \";\").csv(path)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5cd5cdf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---------+\n",
      "| name|age|      job|\n",
      "+-----+---+---------+\n",
      "|Jorge| 30|Developer|\n",
      "|  Bob| 32|Developer|\n",
      "+-----+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read a csv with delimiter and a header\n",
    "df = spark.read.option(\"delimiter\", \";\").option(\"header\", True).csv(path)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4be90222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---------+\n",
      "| name|age|      job|\n",
      "+-----+---+---------+\n",
      "|Jorge| 30|Developer|\n",
      "|  Bob| 32|Developer|\n",
      "+-----+---+---------+\n",
      "\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# You can also use options() to use multiple options\n",
    "df = spark.read.options(delimiter=\";\", header=True).csv(path)\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9c1333",
   "metadata": {},
   "source": [
    "#### JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1fc940de",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"path-to-people.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37fd38a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|NULL|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Spark SQL provides spark.read().json(\"file_name\") to read a file or directory of files in JSON format into Spark DataFrame\n",
    "# It can automatically infer the schema of a JSON dataset\n",
    "df = spark.read.json(path)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f845a8e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## The inferred schema can be visualized using the printSchema() method\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68d4df0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----+\n",
      "|         address|name|\n",
      "+----------------+----+\n",
      "|{Columbus, Ohio}| Yin|\n",
      "+----------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Alternatively, a DataFrame can be created for a JSON dataset represented by\n",
    "# an RDD[String] storing one JSON object per string\n",
    "jsonStrings = ['{\"name\":\"Yin\",\"address\":{\"city\":\"Columbus\",\"state\":\"Ohio\"}}']\n",
    "rdd = sc.parallelize(jsonStrings)\n",
    "df = spark.read.json(rdd)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fafa1f",
   "metadata": {},
   "source": [
    "### sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed3f6d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the DataFrame as a SQL temporary view\n",
    "df.createOrReplaceTempView(\"people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5ee112e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----+\n",
      "|         address|name|\n",
      "+----------------+----+\n",
      "|{Columbus, Ohio}| Yin|\n",
      "+----------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlDF = spark.sql(\"SELECT * FROM people\")\n",
    "sqlDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b73d617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----+\n",
      "|         address|name|\n",
      "+----------------+----+\n",
      "|{Columbus, Ohio}| Yin|\n",
      "+----------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Temporary views in Spark SQL are session-scoped and will disappear if the session that creates it terminates. \n",
    "# If you want to have a temporary view that is shared among all sessions and keep alive until the Spark application terminates, \n",
    "# you can create a global temporary view. Global temporary view is tied to a system preserved database global_temp, \n",
    "# and we must use the qualified name to refer it, e.g. SELECT * FROM global_temp.view1.\n",
    "\n",
    "# Register the DataFrame as a global temporary view\n",
    "df.createGlobalTempView(\"other_people\")\n",
    "\n",
    "# Global temporary view is tied to a system preserved database `global_temp`\n",
    "spark.sql(\"SELECT * FROM global_temp.other_people\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a6b23716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----+\n",
      "|         address|name|\n",
      "+----------------+----+\n",
      "|{Columbus, Ohio}| Yin|\n",
      "+----------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Global temporary view is cross-session\n",
    "spark.newSession().sql(\"SELECT * FROM global_temp.other_people\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761b24dc",
   "metadata": {},
   "source": [
    "## DataFrame Transformation Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9891cd",
   "metadata": {},
   "source": [
    "### select(*cols)\n",
    "Projects a set of expressions and returns a new DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "501feb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2262422d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|age| name|\n",
      "+---+-----+\n",
      "|  2|Alice|\n",
      "|  5|  Bob|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select all columns in the DataFrame\n",
    "df.select('*').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "667cf93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| name|age|\n",
      "+-----+---+\n",
      "|Alice| 12|\n",
      "|  Bob| 15|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select a column with other expressions in the DataFrame\n",
    "df.select(df.name, (df.age + 10).alias('age')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625ad07c",
   "metadata": {},
   "source": [
    "### selectExpr(*expr)\n",
    "Projects a set of SQL expressions and returns a new DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "67536b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|double_age|abs(age)|\n",
      "+----------+--------+\n",
      "|         4|       2|\n",
      "|        10|       5|\n",
      "+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This is a variant of select() that accepts SQL expressions\n",
    "df.selectExpr(\"age * 2 as double_age\", \"abs(age)\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f561e014",
   "metadata": {},
   "source": [
    "### filter(condition)\n",
    "Filters rows using the given condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ba606884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "|  5| Bob|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter by Column instances\n",
    "df.filter(df.age > 3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "933ddfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# where() is an alias for filter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "19d9e9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|age| name|\n",
      "+---+-----+\n",
      "|  2|Alice|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(df.age == 2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "72fc301b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by SQL expression in a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "49cad3b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "|  5| Bob|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(\"age > 3\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05d3536",
   "metadata": {},
   "source": [
    "### drop(*cols)\n",
    "Returns a new DataFrame without specified columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8a9788b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "| name|\n",
      "+-----+\n",
      "|Alice|\n",
      "|  Bob|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.drop('age').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e5e878ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "| name|\n",
      "+-----+\n",
      "|Alice|\n",
      "|  Bob|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.drop(df.age).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be64214",
   "metadata": {},
   "source": [
    "### groupBy(*cols)\n",
    "Groups the DataFrame using the specified columns, so we can run aggregation on them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "abbac5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([(2, \"Alice\"), (2, \"Bob\"), (2, \"Bob\"), (5, \"Bob\")], schema=[\"age\", \"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "93d20f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+\n",
      "| name|sum(age)|\n",
      "+-----+--------+\n",
      "|Alice|       2|\n",
      "|  Bob|       9|\n",
      "+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Group-by ‘name’, and specify a dictionary to calculate the summation of ‘age’\n",
    "df.groupBy(\"name\").agg({\"age\": \"sum\"}).sort(\"name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "665739d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+\n",
      "| name|max(age)|\n",
      "+-----+--------+\n",
      "|Alice|       2|\n",
      "|  Bob|       5|\n",
      "+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Group-by ‘name’, and calculate maximum values\n",
    "df.groupBy(df.name).max().sort(\"name\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e04215",
   "metadata": {},
   "source": [
    "### join(other[, on, how])\n",
    "Joins with another DataFrame, using the given join expression\n",
    "https://images.datacamp.com/image/upload/v1679944054/Marketing/Blog/Joining_Data_in_SQL_Cheat_Sheet.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "199021f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import desc\n",
    "df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")]).toDF(\"age\", \"name\")\n",
    "df2 = spark.createDataFrame([Row(height=80, name=\"Tom\"), Row(height=85, name=\"Bob\")])\n",
    "df3 = spark.createDataFrame([Row(age=2, name=\"Alice\"), Row(age=5, name=\"Bob\")])\n",
    "df4 = spark.createDataFrame([\n",
    "    Row(age=10, height=80, name=\"Alice\"),\n",
    "    Row(age=5, height=None, name=\"Bob\"),\n",
    "    Row(age=None, height=None, name=\"Tom\"),\n",
    "    Row(age=None, height=None, name=None),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e271da2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "|name|height|\n",
      "+----+------+\n",
      "| Bob|    85|\n",
      "+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inner join on columns\n",
    "df.join(df2, 'name').select(df.name, df2.height).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6a9ec206",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "| name|height|\n",
      "+-----+------+\n",
      "|  Tom|    80|\n",
      "|  Bob|    85|\n",
      "|Alice|  NULL|\n",
      "+-----+------+\n",
      "\n",
      "+-----+------+\n",
      "| name|height|\n",
      "+-----+------+\n",
      "|  Bob|    85|\n",
      "|Alice|  NULL|\n",
      "| NULL|    80|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Outer join for both DataFrames on the ‘name’ column\n",
    "df.join(df2, 'name', 'outer').select('name', 'height').sort(desc(\"name\")).show()\n",
    "df.join(df2, df.name == df2.name, 'outer').select(\n",
    "    df.name, df2.height).sort(desc(\"name\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98408f86",
   "metadata": {},
   "source": [
    "### withColumn(colName, col)\n",
    "Returns a new DataFrame by adding a column or replacing the existing column that has the same name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "217774d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----+\n",
      "|age| name|age2|\n",
      "+---+-----+----+\n",
      "|  2|Alice|   4|\n",
      "|  5|  Bob|   7|\n",
      "+---+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
    "df.withColumn('age2', df.age + 2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de87a8f",
   "metadata": {},
   "source": [
    "### agg(*exprs)\n",
    "Aggregate on the entire DataFrame without groups (shorthand for df.groupBy().agg())\n",
    "\n",
    "Available predefined aggregate functions:\n",
    "\n",
    "- avg(*cols) Computes average values for each numeric columns for each group.\n",
    "- count() Counts the number of records for each group.\n",
    "- max(*cols) Computes the max value for each numeric columns for each group.\n",
    "- mean(*cols) Computes average values for each numeric columns for each group.\n",
    "- min(*cols) Computes the min value for each numeric column for each group.\n",
    "- sum(*cols) Computes the sum for each numeric columns for each group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a9c5f037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|max(age)|\n",
      "+--------+\n",
      "|       5|\n",
      "+--------+\n",
      "\n",
      "+--------+\n",
      "|max(age)|\n",
      "+--------+\n",
      "|       5|\n",
      "+--------+\n",
      "\n",
      "+--------+\n",
      "|max(age)|\n",
      "+--------+\n",
      "|       5|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as func\n",
    "df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
    "df.agg({\"age\": \"max\"}).show()\n",
    "df.agg(func.max(\"age\")).show()\n",
    "df.agg(func.max(df.age)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d125b9",
   "metadata": {},
   "source": [
    "### orderBy(*cols, **kwargs)\n",
    "Returns a new DataFrame sorted by the specified column(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ed89e767",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import desc, asc\n",
    "df = spark.createDataFrame([\n",
    "    (2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9b4ac559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|age| name|\n",
      "+---+-----+\n",
      "|  5|  Bob|\n",
      "|  2|Alice|\n",
      "+---+-----+\n",
      "\n",
      "+---+-----+\n",
      "|age| name|\n",
      "+---+-----+\n",
      "|  5|  Bob|\n",
      "|  2|Alice|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sort the DataFrame in descending order\n",
    "df.orderBy(df.age.desc()).show()\n",
    "df.orderBy(desc(\"age\"), \"name\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55071485",
   "metadata": {},
   "source": [
    "## DataFrame Action Methods\n",
    "- show([n, truncate, vertical]): Prints the first n rows to the console\n",
    "- count(): Returns the number of rows in this DataFrame\n",
    "- collect(): Returns all the records as a list of Row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f896852",
   "metadata": {},
   "source": [
    "## DataFrame Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c66cf7f",
   "metadata": {},
   "source": [
    "### columns\n",
    "Retrieves the names of all columns in the DataFrame as a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "06656b1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age', 'name', 'state']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame(\n",
    "    [(14, \"Tom\", \"CA\"), (23, \"Alice\", \"NY\"), (16, \"Bob\", \"TX\")],\n",
    "    [\"age\", \"name\", \"state\"]\n",
    ")\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07c9018",
   "metadata": {},
   "source": [
    "### rdd\n",
    "Returns the content as an pyspark.RDD of Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "95097472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.range(1)\n",
    "type(df.rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd3830d",
   "metadata": {},
   "source": [
    "### schema\n",
    "Returns the schema of this DataFrame as a pyspark.sql.types.StructType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cc7b7026",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "64c81710",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('age', LongType(), True), StructField('name', StringType(), True)])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve the schema of the current DataFrame\n",
    "df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7f2653",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f932cb",
   "metadata": {},
   "source": [
    "## lit(col)\n",
    "Creates a Column of literal value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fef3a344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|height| id|\n",
      "+------+---+\n",
      "|     5|  0|\n",
      "|     5|  1|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "df = spark.range(2)\n",
    "df.select(lit(5).alias('height'), df.id).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c265a5c5",
   "metadata": {},
   "source": [
    "## col\n",
    "Returns a Column based on the given column name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d95983d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df.select(col('id')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787f2f31",
   "metadata": {},
   "source": [
    "## concat(*cols)\n",
    "Concatenates multiple input columns together into a single column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "58608ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|     sd|\n",
      "+-------+\n",
      "|abcd123|\n",
      "+-------+\n",
      "\n",
      "+-------+\n",
      "|     sd|\n",
      "+-------+\n",
      "|abcd123|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import concat, col\n",
    "df = spark.createDataFrame([('abcd','123')], ['s', 'd'])\n",
    "df.select(concat(df.s, df.d).alias('sd')).show()\n",
    "df.select(concat(col('s'), col('d')).alias('sd')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1abdf2e",
   "metadata": {},
   "source": [
    "## udf([f, returnType, useArrow])\n",
    "Creates a user defined function (UDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "49923705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+--------+\n",
      "|age| name|maturity|\n",
      "+---+-----+--------+\n",
      "|  1|Alice|   child|\n",
      "+---+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType \n",
    "from pyspark.sql.functions import udf\n",
    "maturity_udf = udf(lambda age: \"adult\" if age >=18 else \"child\", StringType())\n",
    "df = spark.createDataFrame([{'name': 'Alice', 'age': 1}]) \n",
    "df.withColumn(\"maturity\", maturity_udf(df.age)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307c80f5",
   "metadata": {},
   "source": [
    "# Stop SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "06e10520",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
