{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual diagrams depicting the Spark API Created by Jeff Thomspon, https://github.com/jkthompson/pyspark-pictures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "sc = SparkContext(\"local\")\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.collect\">\n",
    "<img align=left src=\"images/pyspark-page22.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**collect()**. Return a list that contains all of the elements in this RDD.The data is transfered from the Spark Java Core to Python enviroment. It is an expensive opperation.\n",
    "\n",
    "Note: This method should only be used if the resulting array is expected to be small, as all the data is loaded into the driver’s memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:274\n",
      "[1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "# collect\n",
    "x = sc.parallelize([1,2,3])\n",
    "y = x.collect()\n",
    "print(x)  # distributed\n",
    "print(y)  # not distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.top\">\n",
    "<img align=left src=\"images/pyspark-page37.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**top(num, key=None)**. Get the top N elements from a RDD. Note: It returns the list sorted in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 1, 2, 3]\n",
      "[3, 3, 2]\n"
     ]
    }
   ],
   "source": [
    "# top\n",
    "x = sc.parallelize([1,3,1,2,3])\n",
    "y = x.top(num = 3)\n",
    "print(x.collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.takeOrdered\">\n",
    "<img align=left src=\"images/pyspark-page38.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**takeOrdered(num, key=None)**. Get the N elements from a RDD ordered in ascending order or as specified by the optional key function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 1, 2, 3]\n",
      "[1, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "# takeOrdered\n",
    "x = sc.parallelize([1,3,1,2,3])\n",
    "y = x.takeOrdered(num = 3)\n",
    "print(x.collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.take\">\n",
    "<img align=left src=\"images/pyspark-page39.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**take(num)**. Take the first num elements of the RDD. It works by first scanning one partition, and use the results from that partition to estimate the number of additional partitions needed to satisfy the limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 1, 2, 3]\n",
      "[1, 3, 1]\n"
     ]
    }
   ],
   "source": [
    "# take\n",
    "x = sc.parallelize([1,3,1,2,3])\n",
    "y = x.take(num = 3)\n",
    "print(x.collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.first\">\n",
    "<img align=left src=\"images/pyspark-page40.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**first()**. Return the first element in this RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 1, 2, 3]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# first\n",
    "x = sc.parallelize([1,3,1,2,3])\n",
    "y = x.first()\n",
    "print(x.collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.collectAsMap\">\n",
    "<img align=left src=\"images/pyspark-page41.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**collectAsMap()**. Return the key-value pairs in this RDD to the master as a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('C', 3), ('A', 1), ('B', 2)]\n",
      "{'C': 3, 'A': 1, 'B': 2}\n"
     ]
    }
   ],
   "source": [
    "# collectAsMap\n",
    "x = sc.parallelize([('C',3),('A',1),('B',2)])\n",
    "y = x.collectAsMap()\n",
    "print(x.collect())\n",
    "print(y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.map\">\n",
    "<img align=left src=\"images/pyspark-page3.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**map(f, preservesPartitioning=False)**. Return a new distributed dataset formed by passing each element of the source through a function func."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['b', 'a', 'c']\n",
      "[('b', 1), ('a', 1), ('c', 1)]\n"
     ]
    }
   ],
   "source": [
    "x = sc.parallelize([\"b\", \"a\", \"c\"])\n",
    "y = x.map(lambda x: (x, 1))\n",
    "print(x.collect())  # collect copies RDD elements to a list on the driver\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n",
      "[(1, 1), (2, 4), (3, 9)]\n"
     ]
    }
   ],
   "source": [
    "# map\n",
    "x = sc.parallelize([1,2,3]) # sc = spark context, parallelize creates an RDD from the passed object\n",
    "y = x.map(lambda x: (x,x**2))\n",
    "print(x.collect())  # collect copies RDD elements to a list on the driver\n",
    "print(y.collect())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.flatMap\">\n",
    "<img align=left src=\"images/pyspark-page4.svg\" width=360 height=203 /></a>\n",
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**flatMap(f, preservesPartitioning=False)**. Return a new RDD by first applying a function to all elements of this RDD, and then flattening the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n",
      "[1, 100, 1, 2, 200, 4, 3, 300, 9]\n"
     ]
    }
   ],
   "source": [
    "# flatMap\n",
    "x = sc.parallelize([1,2,3])\n",
    "y = x.flatMap(lambda x: (x, 100*x, x**2))\n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n",
      "[(1, 100, 1), (2, 200, 4), (3, 300, 9)]\n"
     ]
    }
   ],
   "source": [
    "# Map\n",
    "x = sc.parallelize([1,2,3])\n",
    "y = x.map(lambda x: (x, 100*x, x**2))\n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 4]\n",
      "[1, 1, 2, 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "x = sc.parallelize([2, 3, 4])\n",
    "y = x.flatMap(lambda x: range(1, x))\n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Apache Spark is a unified analytics engine for large-scale data processing.', 'It provides high-level APIs in Java, Scala, Python and R', 'It also supports a rich set of higher-level tools including Spark SQL', 'MLlib for machine learning', 'GraphX for graph processing', 'Structured Streaming for incremental computation and stream processing']\n",
      "['Apache', 'Spark', 'is', 'a', 'unified', 'analytics', 'engine', 'for', 'large-scale', 'data', 'processing.', 'It', 'provides', 'high-level', 'APIs', 'in', 'Java,', 'Scala,', 'Python', 'and', 'R', 'It', 'also', 'supports', 'a', 'rich', 'set', 'of', 'higher-level', 'tools', 'including', 'Spark', 'SQL', 'MLlib', 'for', 'machine', 'learning', 'GraphX', 'for', 'graph', 'processing', 'Structured', 'Streaming', 'for', 'incremental', 'computation', 'and', 'stream', 'processing']\n"
     ]
    }
   ],
   "source": [
    "# Split sentence into words\n",
    "lines = sc.parallelize([\n",
    "    \"Apache Spark is a unified analytics engine for large-scale data processing.\",\n",
    "    \"It provides high-level APIs in Java, Scala, Python and R\",\n",
    "    \"It also supports a rich set of higher-level tools including Spark SQL\",\n",
    "    \"MLlib for machine learning\",\n",
    "    \"GraphX for graph processing\",\n",
    "    \"Structured Streaming for incremental computation and stream processing\"\n",
    " ])\n",
    "words = lines.flatMap(lambda x: x.split(' '))\n",
    "print(lines.collect())\n",
    "print(words.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 2, 3]\n",
      "[1, 1, 2, 3, 1, 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "x = sc.parallelize([1, 1, 2, 3])\n",
    "y =  x.union(x)\n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.mapValues\">\n",
    "<img align=left src=\"images/pyspark-page56.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**mapValues(f)**. Pass each value in the key-value pair RDD through a map function without changing the keys; this also retains the original RDD’s partitioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('A', (1, 2, 3)), ('B', (4, 5))]\n",
      "[('A', [1, 4, 9]), ('B', [16, 25])]\n"
     ]
    }
   ],
   "source": [
    "# mapValues\n",
    "x = sc.parallelize([('A',(1,2,3)),('B',(4,5))])\n",
    "y = x.mapValues(lambda x: [i**2 for i in x]) # function is applied to entire value\n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.filter\">\n",
    "<img align=left src=\"images/pyspark-page8.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**filter(func)** Return a new dataset formed by selecting those elements of the source on which func returns true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n",
      "[1, 3]\n"
     ]
    }
   ],
   "source": [
    "# filter\n",
    "x = sc.parallelize([1,2,3])\n",
    "y = x.filter(lambda x: x%2 == 1)  # filters out even elements\n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.distinct\">\n",
    "<img align=left src=\"images/pyspark-page9.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**distinct(numPartitions=None)** Return a new RDD containing the distinct elements in this RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'A', 'B']\n",
      "['A', 'B']\n"
     ]
    }
   ],
   "source": [
    "# distinct\n",
    "x = sc.parallelize(['A','A','B'])\n",
    "y = x.distinct()\n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.keys\">\n",
    "<img align=left src=\"images/pyspark-page42.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**keys()**. Return an RDD with the keys of each tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('C', 3), ('A', 1), ('B', 2)]\n",
      "['C', 'A', 'B']\n"
     ]
    }
   ],
   "source": [
    "# keys\n",
    "x = sc.parallelize([('C',3),('A',1),('B',2)])\n",
    "y = x.keys()\n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.values\">\n",
    "<img align=left src=\"images/pyspark-page43.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**values()**. Return an RDD with the values of each tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('C', 3), ('A', 1), ('B', 2)]\n",
      "[3, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "# values\n",
    "x = sc.parallelize([('C',3),('A',1),('B',2)])\n",
    "y = x.values()\n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.getNumPartitions\">\n",
    "<img align=left src=\"images/pyspark-page7.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**getNumPartitions()** Returns the number of partitions in RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12, 13, 14]]\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# getNumPartitions\n",
    "x = sc.parallelize([1,2,3,4,5,6,7,8,9,10,11,12,13,14], 4)\n",
    "y = x.getNumPartitions()\n",
    "print(x.glom().collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.repartition\">\n",
    "<img align=left src=\"images/pyspark-page63.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**repartition(numPartitions)**. Return a new RDD that has exactly numPartitions partitions. Can increase or decrease the level of parallelism in this RDD. Internally, this uses a shuffle to redistribute data. If you are decreasing the number of partitions in this RDD, consider using coalesce, which can avoid performing a shuffle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2], [3, 4, 5]]\n",
      "[[], [1, 2], [3, 4, 5]]\n"
     ]
    }
   ],
   "source": [
    "# repartition\n",
    "x = sc.parallelize([1,2,3,4,5],2)\n",
    "y = x.repartition(numPartitions=3)\n",
    "print(x.glom().collect())\n",
    "print(y.glom().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.coalesce\">\n",
    "<img align=left src=\"images/pyspark-page64.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**coalesce(numPartitions, shuffle=False)**. Return a new RDD that is reduced into numPartitions partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12, 13, 14]]\n",
      "[[1, 2, 3, 4, 5, 6], [7, 8, 9, 10, 11, 12, 13, 14]]\n"
     ]
    }
   ],
   "source": [
    "# coalesce\n",
    "x = sc.parallelize([1,2,3,4,5,6,7,8,9,10,11,12,13,14], 4)\n",
    "y = x.coalesce(numPartitions=2)\n",
    "print(x.glom().collect())\n",
    "print(y.glom().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.glom\">\n",
    "<img align=left src=\"images/pyspark-page16.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**glom()**. Return an RDD created by coalescing all elements within each partition into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C', 'B', 'A']\n",
      "[['C'], ['B', 'A']]\n"
     ]
    }
   ],
   "source": [
    "# glom\n",
    "x = sc.parallelize(['C','B','A'], 2)\n",
    "y = x.glom()\n",
    "print(x.collect()) \n",
    "print(y.collect())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.takeSample\">\n",
    "<img align=left src=\"images/pyspark-page11.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**takeSample(withReplacement, num, seed=None)**. Return a fixed-size sampled subset of this RDD (currently requires numpy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = [0, 1, 2, 3, 4, 5, 6]\n",
      "sample:0 y = [2, 0, 5]\n",
      "sample:1 y = [1, 3, 0]\n",
      "sample:2 y = [6, 5, 4]\n",
      "sample:3 y = [6, 1, 3]\n",
      "sample:4 y = [5, 0, 6]\n"
     ]
    }
   ],
   "source": [
    "# takeSample\n",
    "x = sc.parallelize(range(7))\n",
    "ylist = [x.takeSample(withReplacement=False, num=3) for i in range(5)]  # call 'sample' 5 times\n",
    "print('x = ' + str(x.collect()))\n",
    "for cnt,y in zip(range(len(ylist)), ylist):\n",
    "    print('sample:' + str(cnt) + ' y = ' +  str(y))  # no collect on y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set oprations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.union\">\n",
    "<img align=left src=\"images/pyspark-page12.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**union(other)**. Return the union of this RDD and another one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'A', 'B']\n",
      "['D', 'C', 'A']\n",
      "['A', 'A', 'B', 'D', 'C', 'A']\n"
     ]
    }
   ],
   "source": [
    "# union\n",
    "x = sc.parallelize(['A','A','B'])\n",
    "y = sc.parallelize(['D','C','A'])\n",
    "z = x.union(y)\n",
    "print(x.collect())\n",
    "print(y.collect())\n",
    "print(z.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.intersection\">\n",
    "<img align=left src=\"images/pyspark-page13.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**intersection(other)**. Return the intersection of this RDD and another one. The output will not contain any duplicate elements, even if the input RDDs did. Note that this method performs a shuffle internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'A', 'B']\n",
      "['A', 'C', 'D']\n",
      "['A']\n"
     ]
    }
   ],
   "source": [
    "# intersection\n",
    "x = sc.parallelize(['A','A','B'])\n",
    "y = sc.parallelize(['A','C','D'])\n",
    "z = x.intersection(y)\n",
    "print(x.collect())\n",
    "print(y.collect())\n",
    "print(z.collect())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.reduce\">\n",
    "<img align=left src=\"images/pyspark-page23.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**reduce(f)**. Reduces the elements of this RDD using the specified commutative and associative binary operator. Currently reduces partitions locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "# reduce\n",
    "x = sc.parallelize([1,2,3])\n",
    "y = x.reduce(lambda x, y: x + y)  # computes a cumulative sum\n",
    "print(x.collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.fold\">\n",
    "<img align=left src=\"images/pyspark-page24.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**fold(zeroValue, op)**. Aggregate the elements of each partition, and then the results for all the partitions, using a given associative function and a neutral “zero value\". The function op(t1, t2) is allowed to modify t1 and return it as its result value to avoid object allocation; however, it should not modify t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "# fold\n",
    "x = sc.parallelize([1,2,3])\n",
    "neutral_zero_value = 0  # 0 for sum, 1 for multiplication\n",
    "y = x.fold(neutral_zero_value,lambda obj, accumulated: accumulated + obj) # computes cumulative sum\n",
    "print(x.collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.aggregate\">\n",
    "<img align=left src=\"images/pyspark-page25.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**aggregate(zeroValue, seqOp, combOp)**. Aggregate the elements of each partition, and then the results for all the partitions, using a given combine functions and a neutral “zero value.” \n",
    "\n",
    "The functions op(t1, t2) is allowed to modify t1 and return it as its result value to avoid object allocation; however, it should not modify t2. The first function (seqOp) can return a different result type, U, than the type of this RDD. \n",
    "\n",
    "Thus, we need one operation for merging a T into an U and one operation for merging two U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 4]\n",
      "(9, 24)\n"
     ]
    }
   ],
   "source": [
    "# aggregate\n",
    "x = sc.parallelize([2,3,4])\n",
    "neutral_zero_value = (0,1) # sum: x+0 = x, product: 1*x = x\n",
    "seqOp = (lambda aggregated, el: (aggregated[0] + el, aggregated[1] * el)) \n",
    "combOp = (lambda aggregated, el: (aggregated[0] + el[0], aggregated[1] * el[1]))\n",
    "y = x.aggregate(neutral_zero_value,seqOp,combOp)  # computes (cumulative sum, cumulative product)\n",
    "print(x.collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.reduceByKey\">\n",
    "<img align=left src=\"images/pyspark-page44.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**reduceByKey(func, numPartitions=None)**. Merge the values for each key using an associative reduce function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('B', 1), ('B', 2), ('A', 3), ('A', 4), ('A', 5)]\n",
      "[('B', 3), ('A', 12)]\n"
     ]
    }
   ],
   "source": [
    "# reduceByKey\n",
    "x = sc.parallelize([('B',1),('B',2),('A',3),('A',4),('A',5)])\n",
    "y = x.reduceByKey(lambda agg, obj: agg + obj)\n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.foldByKey\">\n",
    "<img align=left src=\"images/pyspark-page53.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**foldByKey(zeroValue, func, numPartitions=None)**. Merge the values for each key using an associative function “func” and a neutral “zeroValue” which may be added to the result an arbitrary number of times, and must not change the result (e.g., 0 for addition, or 1 for multiplication.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('B', 1), ('B', 2), ('A', 3), ('A', 4), ('A', 5)]\n",
      "[('B', 2), ('A', 60)]\n"
     ]
    }
   ],
   "source": [
    "# foldByKey\n",
    "x = sc.parallelize([('B',1),('B',2),('A',3),('A',4),('A',5)])\n",
    "zeroValue = 1 # one is 'zero value' for multiplication\n",
    "y = x.foldByKey(zeroValue,lambda agg,x: agg*x )  # computes cumulative product within each key\n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.aggregateByKey\">\n",
    "<img align=left src=\"images/pyspark-page52.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**aggregateByKey(zeroValue, seqFunc, combFunc, numPartitions=None)**. Aggregate the values of each key, using given combine functions and a neutral “zero value”. This function can return a different result type, U, than the type of the values in this RDD, V. Thus, we need one operation for merging a V into a U and one operation for merging two U’s, The former operation is used for merging values within a partition, and the latter is used for merging values between partitions. To avoid memory allocation, both of these functions are allowed to modify and return their first argument instead of creating a new U."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('B', 1), ('B', 2), ('A', 3), ('A', 4), ('A', 5)]\n",
      "[('B', [(1, 1), (2, 4)]), ('A', [(3, 9), (4, 16), (5, 25)])]\n"
     ]
    }
   ],
   "source": [
    "# aggregateByKey\n",
    "x = sc.parallelize([('B',1),('B',2),('A',3),('A',4),('A',5)])\n",
    "zeroValue = [] # empty list is 'zero value' for append operation\n",
    "mergeVal = (lambda aggregated, el: aggregated + [(el,el**2)])\n",
    "mergeComb = (lambda agg1,agg2: agg1 + agg2 )\n",
    "y = x.aggregateByKey(zeroValue,mergeVal,mergeComb)\n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.groupByKey\">\n",
    "<img align=left src=\"images/pyspark-page54.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('B', 5), ('B', 4), ('A', 3), ('A', 2), ('A', 1)]\n",
      "[('B', [5, 4]), ('A', [3, 2, 1])]\n"
     ]
    }
   ],
   "source": [
    "# groupByKey\n",
    "x = sc.parallelize([('B',5),('B',4),('A',3),('A',2),('A',1)])\n",
    "y = x.groupByKey()\n",
    "print(x.collect())\n",
    "print([(j[0],[i for i in j[1]]) for j in y.collect()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.countByKey\">\n",
    "<img align=left src=\"images/pyspark-page46.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**countByKey()**. Count the number of elements for each key, and return the result to the master as a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('B', 1), ('B', 2), ('A', 3), ('A', 4), ('A', 5)]\n",
      "defaultdict(<class 'int'>, {'B': 2, 'A': 3})\n"
     ]
    }
   ],
   "source": [
    "# countByKey\n",
    "x = sc.parallelize([('B',1),('B',2),('A',3),('A',4),('A',5)])\n",
    "y = x.countByKey()\n",
    "print(x.collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.countByValue\">\n",
    "<img align=left src=\"images/pyspark-page36.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**countByValue()**. Return the count of each unique value in this RDD as a dictionary of (value, count) pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 1, 2, 3]\n",
      "defaultdict(<class 'int'>, {1: 2, 3: 2, 2: 1})\n"
     ]
    }
   ],
   "source": [
    "# countByValue\n",
    "x = sc.parallelize([1,3,1,2,3])\n",
    "y = x.countByValue()\n",
    "print(x.collect())\n",
    "print(y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.max\">\n",
    "<img align=left src=\"images/pyspark-page26.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**max(key=None)**. Find the maximum item in this RDD.\n",
    "\n",
    "Parameters:\tkey – A function used to generate key for comparing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 2, 11]\n",
      "11\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# max\n",
    "x = sc.parallelize([1,3,2,11])\n",
    "y = x.max()\n",
    "z = x.max(key=str)\n",
    "print(x.collect())\n",
    "print(y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.min\">\n",
    "<img align=left src=\"images/pyspark-page27.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**min(key=None)**. Find the minimum item in this RDD.\n",
    "\n",
    "Parameters:\tkey – A function used to generate key for comparin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 2]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# min\n",
    "x = sc.parallelize([1,3,2])\n",
    "y = x.min()\n",
    "print(x.collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.sum\">\n",
    "<img align=left src=\"images/pyspark-page28.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**sum()**. Add up the elements in this RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 2]\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "# sum\n",
    "x = sc.parallelize([1,3,2])\n",
    "y = x.sum()\n",
    "print(x.collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.count\">\n",
    "<img align=left src=\"images/pyspark-page29.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**count()**. Return the number of elements in this RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 2]\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# count\n",
    "x = sc.parallelize([1,3,2])\n",
    "y = x.count()\n",
    "print(x.collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.mean\">\n",
    "<img align=left src=\"images/pyspark-page31.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**mean()**. Compute the mean of this RDD’s elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 2]\n",
      "2.0\n"
     ]
    }
   ],
   "source": [
    "# mean\n",
    "x = sc.parallelize([1,3,2])\n",
    "y = x.mean()\n",
    "print(x.collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.variance\">\n",
    "<img align=left src=\"images/pyspark-page32.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**variance()**. Compute the variance of this RDD’s elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 2]\n",
      "0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "# variance\n",
    "x = sc.parallelize([1,3,2])\n",
    "y = x.variance()  # divides by N\n",
    "print(x.collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.stdev\">\n",
    "<img align=left src=\"images/pyspark-page33.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 2]\n",
      "0.816496580927726\n"
     ]
    }
   ],
   "source": [
    "# stdev\n",
    "x = sc.parallelize([1,3,2])\n",
    "y = x.stdev()  # divides by N\n",
    "print(x.collect())\n",
    "print(y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join and combine RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.join\">\n",
    "<img align=left src=\"images/pyspark-page47.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**join(other, numPartitions=None)**. Return an RDD containing all pairs of elements with matching keys in self and other. Each pair of elements will be returned as a (k, (v1, v2)) tuple, where (k, v1) is in self and (k, v2) is in other. Performs a hash join across the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('C', 4), ('B', 3), ('A', 2), ('A', 1)]\n",
      "[('A', 8), ('B', 7), ('A', 6), ('D', 5)]\n",
      "[('B', (3, 7)), ('A', (2, 8)), ('A', (2, 6)), ('A', (1, 8)), ('A', (1, 6))]\n"
     ]
    }
   ],
   "source": [
    "# join\n",
    "x = sc.parallelize([('C',4),('B',3),('A',2),('A',1)])\n",
    "y = sc.parallelize([('A',8),('B',7),('A',6),('D',5)])\n",
    "z = x.join(y)\n",
    "print(x.collect())\n",
    "print(y.collect())\n",
    "print(z.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.leftOuterJoin\">\n",
    "<img align=left src=\"images/pyspark-page48.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**leftOuterJoin(other, numPartitions=None)**. Perform a left outer join of self and other. For each element (k, v) in self, the resulting RDD will either contain all pairs (k, (v, w)) for w in other, or the pair (k, (v, None)) if no elements in other have key k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('C', 4), ('B', 3), ('A', 2), ('A', 1)]\n",
      "[('A', 8), ('B', 7), ('A', 6), ('D', 5)]\n",
      "[('C', (4, None)), ('B', (3, 7)), ('A', (2, 8)), ('A', (2, 6)), ('A', (1, 8)), ('A', (1, 6))]\n"
     ]
    }
   ],
   "source": [
    "# leftOuterJoin\n",
    "x = sc.parallelize([('C',4),('B',3),('A',2),('A',1)])\n",
    "y = sc.parallelize([('A',8),('B',7),('A',6),('D',5)])\n",
    "z = x.leftOuterJoin(y)\n",
    "print(x.collect())\n",
    "print(y.collect())\n",
    "print(z.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.rightOuterJoin\">\n",
    "<img align=left src=\"images/pyspark-page49.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('C', 4), ('B', 3), ('A', 2), ('A', 1)]\n",
      "[('A', 8), ('B', 7), ('A', 6), ('D', 5)]\n",
      "[('B', (3, 7)), ('A', (2, 8)), ('A', (2, 6)), ('A', (1, 8)), ('A', (1, 6)), ('D', (None, 5))]\n"
     ]
    }
   ],
   "source": [
    "# rightOuterJoin\n",
    "x = sc.parallelize([('C',4),('B',3),('A',2),('A',1)])\n",
    "y = sc.parallelize([('A',8),('B',7),('A',6),('D',5)])\n",
    "z = x.rightOuterJoin(y)\n",
    "print(x.collect())\n",
    "print(y.collect())\n",
    "print(z.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.cartesian\">\n",
    "<img align=left src=\"images/pyspark-page17.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**cartesian(other)**. Return the Cartesian product of this RDD and another one, that is, the RDD of all pairs of elements (a, b) where a is in self and b is in other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'B']\n",
      "['C', 'D']\n",
      "[('A', 'C'), ('A', 'D'), ('B', 'C'), ('B', 'D')]\n"
     ]
    }
   ],
   "source": [
    "# cartesian\n",
    "x = sc.parallelize(['A','B'])\n",
    "y = sc.parallelize(['C','D'])\n",
    "z = x.cartesian(y)\n",
    "print(x.collect())\n",
    "print(y.collect())\n",
    "print(z.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.subtract\">\n",
    "<img align=left src=\"images/pyspark-page61.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**subtract(other, numPartitions=None)**. Return each value in self that is not contained in other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('C', 4), ('B', 3), ('A', 2), ('A', 1)]\n",
      "[('C', 8), ('A', 2), ('D', 1)]\n",
      "[('C', 4), ('B', 3), ('A', 1)]\n"
     ]
    }
   ],
   "source": [
    "# subtract\n",
    "x = sc.parallelize([('C',4),('B',3),('A',2),('A',1)])\n",
    "y = sc.parallelize([('C',8),('A',2),('D',1)])\n",
    "z = x.subtract(y)\n",
    "print(x.collect())\n",
    "print(y.collect())\n",
    "print(z.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.zip\">\n",
    "<img align=left src=\"images/pyspark-page65.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**zip(other)**.Zips this RDD with another one, returning key-value pairs with the first element in each RDD second element in each RDD, etc. Assumes that the two RDDs have the same number of partitions and the same number of elements in each partition (e.g. one was made through a map on the other)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B', 'A', 'A']\n",
      "[66, 65, 65]\n",
      "[('B', 66), ('A', 65), ('A', 65)]\n"
     ]
    }
   ],
   "source": [
    "# zip\n",
    "x = sc.parallelize(['B','A','A'])\n",
    "y = x.map(lambda x: ord(x))  # zip expects x and y to have same #partitions and #elements/partition\n",
    "z = x.zip(y)\n",
    "print(x.collect())\n",
    "print(y.collect())\n",
    "print(z.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.zipWithIndex\">\n",
    "<img align=left src=\"images/pyspark-page66.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**zipWithIndex()**. Zips this RDD with its element indices. The ordering is first based on the partition index and then the ordering of items within each partition. So the first item in the first partition gets index 0, and the last item in the last partition receives the largest index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['B'], ['A', 'A']]\n",
      "[('B', 0), ('A', 1), ('A', 2)]\n"
     ]
    }
   ],
   "source": [
    "# zipWithIndex\n",
    "x = sc.parallelize(['B','A','A'],2)\n",
    "y = x.zipWithIndex()\n",
    "print(x.glom().collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.sortByKey\">\n",
    "<img align=left src=\"images/pyspark-page14.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**sortByKey(ascending=True, numPartitions=None, keyfunc=lambda)**. Sorts this RDD, which is assumed to consist of (key, value) pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('B', 1), ('A', 2), ('C', 3)]\n",
      "[('A', 2), ('B', 1), ('C', 3)]\n"
     ]
    }
   ],
   "source": [
    "# sortByKey\n",
    "x = sc.parallelize([('B',1),('A',2),('C',3)])\n",
    "y = x.sortByKey()\n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.foreach\">\n",
    "<img align=left src=\"images/pyspark-page20.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**foreach(f)**. Applies a function to all elements of this RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n",
      "None\n",
      "1\n",
      "2\n",
      "3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# foreach\n",
    "from __future__ import print_function\n",
    "x = sc.parallelize([1,2,3])\n",
    "def f(el):\n",
    "    '''side effect: append the current RDD elements to a file'''\n",
    "    f1=open(\"./foreachExample.txt\", 'a+') \n",
    "    print(el,file=f1)\n",
    "\n",
    "open('./foreachExample.txt', 'w').close()  # first clear the file contents\n",
    "\n",
    "y = x.foreach(f) # writes into foreachExample.txt\n",
    "\n",
    "print(x.collect())\n",
    "print(y) # foreach returns 'None'\n",
    "# print the contents of foreachExample.txt\n",
    "with open(\"./foreachExample.txt\", \"r\") as foreachExample:\n",
    "    print (foreachExample.read())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "9db6cbf0fd79f8e79653fe7b0c50b956ca6e525ee712295da3c66f75e4fe96ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
