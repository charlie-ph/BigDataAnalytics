{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm Examples in Python vs PySpark\n",
    "The objective of this Notebook is to help students understand the differences between implementing simple algorithms in Python and PySpark. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "sc = SparkContext(\"local\")\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sum of elements in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "# Python code\n",
    "arr = [1, 2, 3, 4, 5]\n",
    "result = 0\n",
    "for i in range(len(arr)):\n",
    "    result += arr[i]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "# PySpark code\n",
    "rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
    "result = rdd.reduce(lambda a, b: a + b)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hello': 2, 'world': 1}\n"
     ]
    }
   ],
   "source": [
    "# Python code\n",
    "text = \"hello world hello\"\n",
    "word_dict = {}\n",
    "for word in text.split():\n",
    "    word_dict[word] = word_dict.get(word, 0) + 1\n",
    "print(word_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hello': 2, 'world': 1}\n"
     ]
    }
   ],
   "source": [
    "# PySpark code\n",
    "text = sc.parallelize([\"hello world hello\"])\n",
    "result = (text.flatMap(lambda line: line.split(\" \"))\n",
    "          .map(lambda word: (word, 1))\n",
    "          .reduceByKey(lambda a, b: a + b)\n",
    "          .collect())\n",
    "\n",
    "print(dict(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Maximum Element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "# Python code\n",
    "arr = [2, 4, 1, 8, 5]\n",
    "max_val = arr[0]\n",
    "for i in range(1, len(arr)):\n",
    "    if arr[i] > max_val:\n",
    "        max_val = arr[i]\n",
    "print(max_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "# PySpark code\n",
    "rdd = sc.parallelize([2, 4, 1, 8, 5])\n",
    "result = rdd.reduce(lambda a, b: a if a > b else b)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting Occurrences of a Specific Element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# Python code\n",
    "arr = [2, 3, 4, 2, 8, 2]\n",
    "target = 2\n",
    "\n",
    "result = 0\n",
    "for i in range(len(arr)):\n",
    "    if arr[i] == target:\n",
    "        result += 1\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# PySpark code\n",
    "rdd = sc.parallelize([2, 3, 4, 2, 8, 2])\n",
    "target = 2\n",
    "result = rdd.filter(lambda x: x == target).count()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Prime Numbers in a Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 5, 7, 11, 13, 17, 19, 23, 29]\n"
     ]
    }
   ],
   "source": [
    "# Python code\n",
    "\n",
    "def is_prime(n):\n",
    "    if n <= 1:\n",
    "        return False\n",
    "    for i in range(2, int(n ** 0.5)+1):\n",
    "        if n % i == 0:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "prime_numbers = []\n",
    "for i in range(2, 30):\n",
    "    if is_prime(i):\n",
    "        prime_numbers.append(i)\n",
    "print(prime_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 5, 7, 11, 13, 17, 19, 23, 29]\n"
     ]
    }
   ],
   "source": [
    "# PySpark code\n",
    "\n",
    "rdd = sc.parallelize(range(2, 30))\n",
    "prime_numbers = rdd.filter(is_prime).collect()\n",
    "print(prime_numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Histogram Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 3, 2: 2, 3: 1}\n"
     ]
    }
   ],
   "source": [
    "arr = [1, 1, 1, 2, 2, 3]\n",
    "result = {}\n",
    "for i in range(len(arr)):\n",
    "    result[arr[i]] = result.get(arr[i], 0) + 1\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {1: 3, 2: 2, 3: 1})\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([1, 1, 1, 2, 2, 3])\n",
    "result = rdd.countByValue()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sum of Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n"
     ]
    }
   ],
   "source": [
    "# Python code\n",
    "arr = [1, 2, 3, 4, 5]\n",
    "sum_of_squares = 0\n",
    "for i in range(len(arr)):\n",
    "    sum_of_squares += arr[i] ** 2\n",
    "print(sum_of_squares)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n"
     ]
    }
   ],
   "source": [
    "# PySpark code\n",
    "rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
    "sum_of_squares = rdd.map(lambda x: x ** 2).reduce(lambda a, b: a + b)\n",
    "print(sum_of_squares)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF (Term Frequency-Inverse Document Frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'apple': 0.8109302162163288, 'orange': 0.4054651081081644}, {'apple': 0.4054651081081644, 'lemon': 0.4054651081081644}, {'orange': 0.4054651081081644, 'lemon': 0.4054651081081644}]\n"
     ]
    }
   ],
   "source": [
    "# Python code\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "documents = [\"apple orange apple\", \"apple lemon\", \"orange lemon\"]\n",
    "N = len(documents)\n",
    "idf_dict = Counter()\n",
    "tf_dict_list = []\n",
    "\n",
    "for doc in documents:\n",
    "    tf_dict = Counter(doc.split())\n",
    "    tf_dict_list.append(tf_dict)\n",
    "    for word in tf_dict.keys():\n",
    "        idf_dict[word] += 1\n",
    "        \n",
    "for word in idf_dict.keys():\n",
    "    idf_dict[word] = math.log(N / idf_dict[word])\n",
    "    \n",
    "tfidf_documents = []\n",
    "\n",
    "for tf_dict in tf_dict_list:\n",
    "    tfidf = {}\n",
    "    for word, count in tf_dict.items():\n",
    "        tfidf[word] = count * idf_dict[word]\n",
    "    tfidf_documents.append(tfidf)\n",
    "\n",
    "print(tfidf_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'apple': 0.8109302162163288, 'orange': 0.4054651081081644}, {'apple': 0.4054651081081644, 'lemon': 0.4054651081081644}, {'orange': 0.4054651081081644, 'lemon': 0.4054651081081644}]\n"
     ]
    }
   ],
   "source": [
    "# PySpark code\n",
    "documents = [\"apple orange apple\", \"apple lemon\", \"orange lemon\"]\n",
    "rdd = sc.parallelize(documents)\n",
    "\n",
    "# TF calculation\n",
    "tf_rdd = rdd.map(lambda doc: Counter(doc.split()))\n",
    "\n",
    "# IDF calculation\n",
    "idf_rdd = tf_rdd.flatMap(lambda tf: tf.keys()).map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "N = rdd.count()\n",
    "idf_values = idf_rdd.mapValues(lambda count: math.log(N / count)).collectAsMap()\n",
    "\n",
    "# TF-IDF calculation\n",
    "tfidf_rdd = tf_rdd.map(lambda tf: {word: tf[word] * idf_values[word] for word in tf})\n",
    "\n",
    "tfidf = tfidf_rdd.collect()\n",
    "print(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
